{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recommender Systems with Graph Neural Networks in PyG\n",
        "\n",
        "By Derrick Li, Peter Maldonado, Akram Sbaih as part of the Stanford CS224W course project.\n",
        "\n",
        "In this tutorial, we implement two GNN recommender system architectures, LightGCN and NGCF, in PyG and apply them to the MovieLens 100K dataset."
      ],
      "metadata": {
        "id": "ktxdLosxtgZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First, we'll install the necessary packages."
      ],
      "metadata": {
        "id": "BoRvTQ1vtwcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Environment Setup (note that capture silences the console output)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "!git clone https://github.com/pmaldonado/cs224w-project-data.git"
      ],
      "metadata": {
        "id": "4p_Pj_D7t3Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's import all of the modules that we'll use in this notebook."
      ],
      "metadata": {
        "id": "u_LPed5cuAEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Third-party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "import torch_geometric\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.sparse as sp"
      ],
      "metadata": {
        "id": "Y9fonQcxt3do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we should double check that our environment is working as expected."
      ],
      "metadata": {
        "id": "nzLUutf7uNAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch_geometric.__version__"
      ],
      "metadata": {
        "id": "J_CDy1cbuF4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "b4pKT5jUt3pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYnQc9UH07Fg"
      },
      "source": [
        "## Dataset and Preprocessing\n",
        "\n",
        "For this tutorial, we’ll be using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/), which contains 100,000 ratings by 943 users of 1682 items (movies). To ensure the quality of the dataset, each user has rated at least 20 movies.\n",
        "\n",
        "We’ll focus on the interactions between users and items, in this case user ratings of movies, but the dataset also provides metadata about users and movies, such as user demographics and movie titles, release dates, and genres. T\n",
        "\n",
        "The user ratings of movies form a bipartite graph, which we can apply graph machine learning methods to recommend new movies to users.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_name=['user_id','item_id','rating','timestamp']\n",
        "df = pd.read_csv(\"./cs224w-project-data/ml-100k/u.data\",sep=\"\\t\",names=columns_name)\n",
        "print(len(df))\n",
        "display(df.head(5))"
      ],
      "metadata": {
        "id": "D13_omigmeOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only want to use high ratings as interactions in order to predict which movies a user will enjoy watching next."
      ],
      "metadata": {
        "id": "BzX3-JClrcCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many ratings are a 3 or above?\n",
        "df = df[df['rating']>=3]\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "j0vvQB9Kmea7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the distribution of highly rated movies?\n",
        "print(\"Rating Distribution\")\n",
        "df.groupby(['rating'])['rating'].count()"
      ],
      "metadata": {
        "id": "yvuk3tEomrQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a 80/20 train-test split on the interactions in the dataset\n",
        "train, test = train_test_split(df.values, test_size=0.2, random_state=16)\n",
        "train_df = pd.DataFrame(train, columns=df.columns)\n",
        "test_df = pd.DataFrame(test, columns=df.columns)"
      ],
      "metadata": {
        "id": "SWwvL8JOmrT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Size  : \", len(train_df))\n",
        "print(\"Test Size : \", len (test_df))"
      ],
      "metadata": {
        "id": "50eSoP3qmrbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we performed the train/test randomly on the interactions, not all users and items may be present in the training set. We will relabel all of users and items to ensure the highest label is the number of users and items, respectively."
      ],
      "metadata": {
        "id": "g0bHVYGjrTcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le_user = pp.LabelEncoder()\n",
        "le_item = pp.LabelEncoder()\n",
        "train_df['user_id_idx'] = le_user.fit_transform(train_df['user_id'].values)\n",
        "train_df['item_id_idx'] = le_item.fit_transform(train_df['item_id'].values)"
      ],
      "metadata": {
        "id": "QXi90opJmriQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_user_ids = train_df['user_id'].unique()\n",
        "train_item_ids = train_df['item_id'].unique()\n",
        "\n",
        "print(len(train_user_ids), len(train_item_ids))\n",
        "\n",
        "test_df = test_df[\n",
        "  (test_df['user_id'].isin(train_user_ids)) & \\\n",
        "  (test_df['item_id'].isin(train_item_ids))\n",
        "]\n",
        "print(len(test))"
      ],
      "metadata": {
        "id": "KtRmOkoDmem_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['user_id_idx'] = le_user.transform(test_df['user_id'].values)\n",
        "test_df['item_id_idx'] = le_item.transform(test_df['item_id'].values)"
      ],
      "metadata": {
        "id": "9fKAfWyCm5eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_users = train_df['user_id_idx'].nunique()\n",
        "n_items = train_df['item_id_idx'].nunique()\n",
        "print(\"Number of Unique Users : \", n_users)\n",
        "print(\"Number of unique Items : \", n_items)"
      ],
      "metadata": {
        "id": "-WOF-cOAm5iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minibatch Sampling\n",
        "\n",
        "Explain the scheme of minibatch positive and negative sample in some amount of prose.\n",
        "\n",
        "We need to add `n_usr` to the sampled positive and negative items, since each node must have a unique id when using PyG."
      ],
      "metadata": {
        "id": "XNoblY5kxlv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(data, batch_size, n_usr, n_itm):\n",
        "\n",
        "    def sample_neg(x):\n",
        "        while True:\n",
        "            neg_id = random.randint(0, n_itm - 1)\n",
        "            if neg_id not in x:\n",
        "                return neg_id\n",
        "\n",
        "    interected_items_df = data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
        "    indices = [x for x in range(n_usr)]\n",
        "\n",
        "    if n_usr < batch_size:\n",
        "        users = [random.choice(indices) for _ in range(batch_size)]\n",
        "    else:\n",
        "        users = random.sample(indices, batch_size)\n",
        "    users.sort()\n",
        "    users_df = pd.DataFrame(users,columns = ['users'])\n",
        "\n",
        "    interected_items_df = pd.merge(interected_items_df, users_df, how = 'right', left_on = 'user_id_idx', right_on = 'users')\n",
        "    pos_items = interected_items_df['item_id_idx'].apply(lambda x : random.choice(x)).values\n",
        "    neg_items = interected_items_df['item_id_idx'].apply(lambda x: sample_neg(x)).values\n",
        "\n",
        "    return (\n",
        "        torch.LongTensor(list(users)).to(device),\n",
        "        torch.LongTensor(list(pos_items)).to(device) + n_usr,\n",
        "        torch.LongTensor(list(neg_items)).to(device) + n_usr\n",
        "    )\n",
        "\n",
        "data_loader(train_df, 16, n_users, n_items)"
      ],
      "metadata": {
        "id": "NQRGy-CJnOkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge Index\n",
        "\n",
        "PyG represents graphs as sparse lists of node pairs. Since our graph is undirected, we need to include each edge twice, once for the edges from the users to the items and vice-versa.\n",
        "\n",
        "Similar to above, we add `n_users` to the item tensor to ensure that every node in the graph has a unique identifier."
      ],
      "metadata": {
        "id": "vjHZg1Eu-MKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u_t = torch.LongTensor(train_df.user_id_idx)\n",
        "i_t = torch.LongTensor(train_df.item_id_idx) + n_users\n",
        "\n",
        "train_edge_index = torch.stack((\n",
        "  torch.cat([u_t, i_t]),\n",
        "  torch.cat([i_t, u_t])\n",
        ")).to(device)\n",
        "train_edge_index"
      ],
      "metadata": {
        "id": "O3BkGyV9pkce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's confirm that the first and last edges match the middle two edges, but with the order of nodes swapped."
      ],
      "metadata": {
        "id": "_RxDUYJ2sXJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index[:,-1], train_edge_index[:, 0]"
      ],
      "metadata": {
        "id": "Mq4NVs0_nOxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index[:, len(train)-1], train_edge_index[:, len(train)]"
      ],
      "metadata": {
        "id": "_gwESDz-qgw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ys1P7mtcr54"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "First, let's take a look at the graph convolutional layers that will power our recommender system GNN. Then, we can implement a wrapper to stack multiple convolutional layers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGCN Convolutional Layer\n",
        "\n",
        "The LightGCN architecture is governed by the following rules:\n",
        "\n",
        "$$e_{u}^{(k+1)} = \\sum\\limits_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|}\\sqrt{|N_i|}}e^{(k)}_i$$\n",
        "\n",
        "$$e_{i}^{(k+1)} = \\sum\\limits_{u \\in N_i} \\frac{1}{\\sqrt{|N_i|}\\sqrt{|N_u|}}e^{(k)}_u$$\n",
        "In essence, the embedding for each node after a single LightGCN layer is the sum of the synthetic normalized embeddings of it's neighbors before the layer."
      ],
      "metadata": {
        "id": "49WD8SryyUds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Briefly explain how the `MessagePassing` class works (look at colabs)\n",
        "\n",
        "We can specify the type of aggregation our `MessagePassing` layer should use by passing in an `aggr=` argument in the layer initialization. Here we use `add` to specify summation aggregation of messages.\n",
        "\n",
        "Note that we could have manually defined our aggregation function by defining a function explicitly in the class:\n",
        "```\n",
        "def aggregate(self, x, messages, index):\n",
        "  return torch_scatter.scatter(messages, index, self.node_dim, reduce=\"sum\")\n",
        "```\n",
        "The `torch_scatter.scatter` function enables us to aggregate messages being sent to the same node. The `reduce=` argument specifies how to aggregate, while `index` has the same length as the `messages` tensor and maps from message to destination node."
      ],
      "metadata": {
        "id": "IcUsEulPtNNp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aTMoHisNIh_"
      },
      "source": [
        "class LightGCNConv(MessagePassing):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(aggr='add')\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    # Compute normalization\n",
        "    from_, to_ = edge_index\n",
        "    deg = degree(to_, x.size(0), dtype=x.dtype)\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "    norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "\n",
        "    # Start propagating messages (no update after aggregation)\n",
        "    return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "  def message(self, x_j, norm):\n",
        "    return norm.view(-1, 1) * x_j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test out our implementation of the LightGCN convolution by applying it to a small bipartite graph.\n",
        "\n",
        "This sample graph is undirected, and node 0 is connected to nodes 2 and 3 while node 1 is connected to 3 and 4."
      ],
      "metadata": {
        "id": "P0Lrwz-4yei9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = torch.Tensor(np.eye(5))\n",
        "test_edge_index = torch.LongTensor(np.array([\n",
        "  [0, 0, 1, 1, 2, 3, 3, 4],\n",
        "  [2, 3, 3, 4, 0, 0, 1, 1]\n",
        "]))\n",
        "\n",
        "LightGCNConv()(test_x, test_edge_index)"
      ],
      "metadata": {
        "id": "bgcrWvgkhxQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how each node has an initial feature vector corresponding to a one-hot encoding at the index of their id.\n",
        "\n",
        "As we expected, node 0 received messages (and so has non-zero features at the corresponding indicies) from nodes 2 and 3. We can easily verify that nodes 1, 2, 3, and 4 also received messages from their precisely neighbors."
      ],
      "metadata": {
        "id": "e3CtZKN-yvIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NGCF Layer\n",
        "\n",
        "NGCF is an older architecture than LightGCN that originated by researchers who applied [Graph Convolutional Networks (GCNs)]() to recommender systems. LightGCN functions the same as NGCF, but removes the learnable linear layers, non-linear activation, and dropout.\n",
        "\n",
        "One layer of NGCF updates user and item embeddings as follows:\n",
        "\n",
        "$$e_{u}^{(k+1)} = \\sigma\\left(W_1 e_u^{(k)} + \\sum\\limits_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|}\\sqrt{|N_i|}}(W_1e^{(k)}_i + W_2(e^{(k)}_i \\odot e^{(k)}_u))\\right)$$\n",
        "\n",
        "$$e_{i}^{(k+1)} = \\sigma\\left(W_1 e_i^{(k)} + \\sum\\limits_{u \\in N_i} \\frac{1}{\\sqrt{|N_i|}\\sqrt{|N_u|}}(W_1e^{(k)}_u + W_2(e^{(k)}_u \\odot e^{(k)}_i))\\right)$$\n",
        "\n",
        "Typically, NGCF is implemented with dropout before the activation and with an activation function $\\sigma$ of LeakyReLU."
      ],
      "metadata": {
        "id": "0BIVMYyiyPHJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u728UyYfOczG"
      },
      "source": [
        "class NGCFConv(MessagePassing):\n",
        "  def __init__(self, latent_dim, dropout, bias=True, **kwargs):\n",
        "    super(NGCFConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.lin_1 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
        "    self.lin_2 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
        "\n",
        "    self.init_parameters()\n",
        "\n",
        "\n",
        "  def init_parameters(self):\n",
        "    nn.init.xavier_uniform_(self.lin_1.weight)\n",
        "    nn.init.xavier_uniform_(self.lin_2.weight)\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    # Compute normalization\n",
        "    from_, to_ = edge_index\n",
        "    deg = degree(to_, x.size(0), dtype=x.dtype)\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "    norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "\n",
        "    # Start propagating messages\n",
        "    out = self.propagate(edge_index, x=(x, x), norm=norm)\n",
        "\n",
        "    # Perform update after aggregation\n",
        "    out += self.lin_1(x)\n",
        "    out = F.dropout(out, self.dropout, self.training)\n",
        "    return F.leaky_relu(out)\n",
        "\n",
        "\n",
        "  def message(self, x_j, x_i, norm):\n",
        "    return norm.view(-1, 1) * (self.lin_1(x_j) + self.lin_2(x_j * x_i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2tW9FJFqNjn"
      },
      "source": [
        "### Recommender System GNN\n",
        "\n",
        "For this tutorial, we will be using the following class, `RecSysGNN` in order to stack the NGCF or LightGCN convolutional layers. Some considerations that can be made for tweaking the models are the number of layers of your model and dropout. The more number of layers you add to the model, the more your model will \"diffuse\" information of recommendations made from nodes that are `n`-hops away in a model that uses `n` layers. Dropout can be tweaked to try out different regularization schemes.\n",
        "\n",
        "Notice that our forward function works differently from most neural networks by forward propagating embeddings for all nodes in the graph. This is because the embeddings for a given node depend on the embeddings of it's `n`-hop neighborhood, so they need to be propagated as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT5LTkI8Ml1c"
      },
      "source": [
        "class RecSysGNN(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      latent_dim,\n",
        "      num_layers,\n",
        "      num_users,\n",
        "      num_items,\n",
        "      model, # 'NGCF' or 'LightGCN'\n",
        "      dropout=0.1 # Only used in NGCF\n",
        "  ):\n",
        "    super(RecSysGNN, self).__init__()\n",
        "\n",
        "    assert (model == 'NGCF' or model == 'LightGCN'), \\\n",
        "        'Model must be NGCF or LightGCN'\n",
        "    self.model = model\n",
        "    self.embedding = nn.Embedding(num_users + num_items, latent_dim)\n",
        "\n",
        "    if self.model == 'NGCF':\n",
        "      self.convs = nn.ModuleList(\n",
        "        NGCFConv(latent_dim, dropout=dropout) for _ in range(num_layers)\n",
        "      )\n",
        "    else:\n",
        "      self.convs = nn.ModuleList(LightGCNConv() for _ in range(num_layers))\n",
        "\n",
        "    self.init_parameters()\n",
        "\n",
        "\n",
        "  def init_parameters(self):\n",
        "    if self.model == 'NGCF':\n",
        "      nn.init.xavier_uniform_(self.embedding.weight, gain=1)\n",
        "    else:\n",
        "      # Authors of LightGCN report higher results with normal initialization\n",
        "      nn.init.normal_(self.embedding.weight, std=0.1)\n",
        "\n",
        "\n",
        "  def forward(self, edge_index):\n",
        "    emb0 = self.embedding.weight\n",
        "    embs = [emb0]\n",
        "\n",
        "    emb = emb0\n",
        "    for conv in self.convs:\n",
        "      emb = conv(x=emb, edge_index=edge_index)\n",
        "      embs.append(emb)\n",
        "\n",
        "    out = (\n",
        "      torch.cat(embs, dim=-1) if self.model == 'NGCF'\n",
        "      else torch.mean(torch.stack(embs, dim=0), dim=0)\n",
        "    )\n",
        "\n",
        "    return emb0, out\n",
        "\n",
        "\n",
        "  def encode_minibatch(self, users, pos_items, neg_items, edge_index):\n",
        "    emb0, out = self(edge_index)\n",
        "    return (\n",
        "        out[users],\n",
        "        out[pos_items],\n",
        "        out[neg_items],\n",
        "        emb0[users],\n",
        "        emb0[pos_items],\n",
        "        emb0[neg_items]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function and metrics\n",
        "\n",
        "We implement both the Bayesian Personalized Ranking loss function for a single minibatch of users, positive items, and negative items, as well as the precision@K and recall@K metrics."
      ],
      "metadata": {
        "id": "dyqEQ6kfCY5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bpr_loss(users, users_emb, pos_emb, neg_emb, user_emb0,  pos_emb0, neg_emb0):\n",
        "  # compute loss from initial embeddings, used for regulization\n",
        "  reg_loss = (1 / 2) * (\n",
        "    user_emb0.norm().pow(2) +\n",
        "    pos_emb0.norm().pow(2)  +\n",
        "    neg_emb0.norm().pow(2)\n",
        "  ) / float(len(users))\n",
        "\n",
        "  # compute BPR loss from user, positive item, and negative item embeddings\n",
        "  pos_scores = torch.mul(users_emb, pos_emb).sum(dim=1)\n",
        "  neg_scores = torch.mul(users_emb, neg_emb).sum(dim=1)\n",
        "\n",
        "  bpr_loss = torch.mean(F.softplus(neg_scores - pos_scores))\n",
        "\n",
        "  return bpr_loss, reg_loss"
      ],
      "metadata": {
        "id": "bwrPmvXPow5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(user_Embed_wts, item_Embed_wts, n_users, n_items, train_data, test_data, K):\n",
        "  test_user_ids = torch.LongTensor(test_data['user_id_idx'].unique())\n",
        "  # compute the score of all user-item pairs\n",
        "  relevance_score = torch.matmul(user_Embed_wts, torch.transpose(item_Embed_wts,0, 1))\n",
        "\n",
        "  # create dense tensor of all user-item interactions\n",
        "  i = torch.stack((\n",
        "    torch.LongTensor(train_df['user_id_idx'].values),\n",
        "    torch.LongTensor(train_df['item_id_idx'].values)\n",
        "  ))\n",
        "  v = torch.ones((len(train_df)), dtype=torch.float64)\n",
        "  interactions_t = torch.sparse.FloatTensor(i, v, (n_users, n_items))\\\n",
        "      .to_dense().to(device)\n",
        "\n",
        "  # mask out training user-item interactions from metric computation\n",
        "  relevance_score = torch.mul(relevance_score, (1 - interactions_t))\n",
        "\n",
        "  # compute top scoring items for each user\n",
        "  topk_relevance_indices = torch.topk(relevance_score, K).indices\n",
        "  topk_relevance_indices_df = pd.DataFrame(topk_relevance_indices.cpu().numpy(),columns =['top_indx_'+str(x+1) for x in range(K)])\n",
        "  topk_relevance_indices_df['user_ID'] = topk_relevance_indices_df.index\n",
        "  topk_relevance_indices_df['top_rlvnt_itm'] = topk_relevance_indices_df[['top_indx_'+str(x+1) for x in range(K)]].values.tolist()\n",
        "  topk_relevance_indices_df = topk_relevance_indices_df[['user_ID','top_rlvnt_itm']]\n",
        "\n",
        "  # measure overlap between recommended (top-scoring) and held-out user-item\n",
        "  # interactions\n",
        "  test_interacted_items = test_data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
        "  metrics_df = pd.merge(test_interacted_items,topk_relevance_indices_df, how= 'left', left_on = 'user_id_idx',right_on = ['user_ID'])\n",
        "  metrics_df['intrsctn_itm'] = [list(set(a).intersection(b)) for a, b in zip(metrics_df.item_id_idx, metrics_df.top_rlvnt_itm)]\n",
        "\n",
        "  metrics_df['recall'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])/len(x['item_id_idx']), axis = 1)\n",
        "  metrics_df['precision'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])/K, axis = 1)\n",
        "\n",
        "  return metrics_df['recall'].mean(), metrics_df['precision'].mean()"
      ],
      "metadata": {
        "id": "oHuXurG8mezC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate models\n",
        "\n",
        "Now that we've implemented both LightGCN and NGCF in PyG, we can train and evaluate their performance!"
      ],
      "metadata": {
        "id": "_qOC3fF9m6cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 64\n",
        "n_layers = 3\n",
        "\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 1024\n",
        "DECAY = 0.0001\n",
        "LR = 0.005\n",
        "K = 20"
      ],
      "metadata": {
        "id": "MZtgfxxIm5nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(model, optimizer, train_df):\n",
        "  loss_list_epoch = []\n",
        "  bpr_loss_list_epoch = []\n",
        "  reg_loss_list_epoch = []\n",
        "\n",
        "  recall_list = []\n",
        "  precision_list = []\n",
        "\n",
        "  for epoch in tqdm(range(EPOCHS)):\n",
        "      n_batch = int(len(train)/BATCH_SIZE)\n",
        "\n",
        "      final_loss_list = []\n",
        "      bpr_loss_list = []\n",
        "      reg_loss_list = []\n",
        "\n",
        "      model.train()\n",
        "      for batch_idx in range(n_batch):\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          users, pos_items, neg_items = data_loader(train_df, BATCH_SIZE, n_users, n_items)\n",
        "          users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0 = model.encode_minibatch(users, pos_items, neg_items, train_edge_index)\n",
        "\n",
        "          bpr_loss, reg_loss = compute_bpr_loss(\n",
        "            users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0\n",
        "          )\n",
        "          reg_loss = DECAY * reg_loss\n",
        "          final_loss = bpr_loss + reg_loss\n",
        "\n",
        "          final_loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          final_loss_list.append(final_loss.item())\n",
        "          bpr_loss_list.append(bpr_loss.item())\n",
        "          reg_loss_list.append(reg_loss.item())\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          _, out = model(train_edge_index)\n",
        "          final_user_Embed, final_item_Embed = torch.split(out, (n_users, n_items))\n",
        "          test_topK_recall,  test_topK_precision = get_metrics(\n",
        "            final_user_Embed, final_item_Embed, n_users, n_items, train_df, test_df, K\n",
        "          )\n",
        "\n",
        "      loss_list_epoch.append(round(np.mean(final_loss_list),4))\n",
        "      bpr_loss_list_epoch.append(round(np.mean(bpr_loss_list),4))\n",
        "      reg_loss_list_epoch.append(round(np.mean(reg_loss_list),4))\n",
        "\n",
        "      recall_list.append(round(test_topK_recall,4))\n",
        "      precision_list.append(round(test_topK_precision,4))\n",
        "\n",
        "  return (\n",
        "    loss_list_epoch,\n",
        "    bpr_loss_list_epoch,\n",
        "    reg_loss_list_epoch,\n",
        "    recall_list,\n",
        "    precision_list\n",
        "  )"
      ],
      "metadata": {
        "id": "B5HB_FX5pdgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and eval LightGCN"
      ],
      "metadata": {
        "id": "Z4xJSiBiznki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightgcn = RecSysGNN(\n",
        "  latent_dim=latent_dim,\n",
        "  num_layers=n_layers,\n",
        "  num_users=n_users,\n",
        "  num_items=n_items,\n",
        "  model='LightGCN'\n",
        ")\n",
        "lightgcn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(lightgcn.parameters(), lr=LR)\n",
        "print(\"Size of Learnable Embedding : \", [x.shape for x in list(lightgcn.parameters())])"
      ],
      "metadata": {
        "id": "eKBv9eXongux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "light_loss, light_bpr, light_reg, light_recall, light_precision = train_and_eval(lightgcn, optimizer, train_df)"
      ],
      "metadata": {
        "id": "iXfsuJlcy3FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_list = [(i+1) for i in range(EPOCHS)]"
      ],
      "metadata": {
        "id": "vCOJY4XST38b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_list, light_loss, label='Total Training Loss')\n",
        "plt.plot(epoch_list, light_bpr, label='BPR Training Loss')\n",
        "plt.plot(epoch_list, light_reg, label='Reg Training Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Z5P2Zf6yT4Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_list, light_recall, label='Recall')\n",
        "plt.plot(epoch_list, light_precision, label='Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metrics')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "I1Quk5mahJ1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and eval NGCF"
      ],
      "metadata": {
        "id": "yl6OL1UPzqju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngcf = RecSysGNN(\n",
        "  latent_dim=latent_dim,\n",
        "  num_layers=n_layers,\n",
        "  num_users=n_users,\n",
        "  num_items=n_items,\n",
        "  model='NGCF'\n",
        ")\n",
        "ngcf.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(ngcf.parameters(), lr=LR)\n",
        "print(\"Size of Learnable Embedding : \", [x.shape for x in list(ngcf.parameters())])"
      ],
      "metadata": {
        "id": "wwO_h55Jzj4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngcf_loss, ngcf_bpr, ngcf_reg, ngcf_recall, ngcf_precision = train_and_eval(ngcf, optimizer, train_df)"
      ],
      "metadata": {
        "id": "KUtZEhAEzjrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_list = [(i+1) for i in range(EPOCHS)]"
      ],
      "metadata": {
        "id": "ENivAkwTzjcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_list, ngcf_loss, label='Total Training Loss')\n",
        "plt.plot(epoch_list, ngcf_bpr, label='BPR Training Loss')\n",
        "plt.plot(epoch_list, ngcf_reg, label='Reg Training Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3MxNmjo-T4mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_list, ngcf_recall, label='Recall')\n",
        "plt.plot(epoch_list, ngcf_precision, label='Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metrics')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "jnVhVglYpd7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare model performance"
      ],
      "metadata": {
        "id": "myUn4-Sr26KT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MY7QibZ6NNJ"
      },
      "source": [
        "max(light_precision), max(light_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(ngcf_precision), max(ngcf_recall)"
      ],
      "metadata": {
        "id": "hbxiFcIe3BAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paper References\n",
        "\n",
        "1. Harper, F. Maxwell, and Konstan, Joseph A. “The MovieLensDatasets: History and Context.” ACM Transactions on Interactive Intelligence Systems (TiiS) 5, 4. 2015.\n",
        "2. He, Xiangnan, et al. “LightGCN: Simplifying and powering graph convolution network for recommendation.” Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2020.\n",
        "3. Wang, Xiang, et al. “Neural graph collaborative filtering.” Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2019.\n",
        "\n",
        "## Code References\n",
        "\n",
        "We thank the authors of the following codebases and notebooks, from which parts of this tutorial were inspired or adapted.\n",
        "\n",
        "- https://www.kaggle.com/dipanjandas96/lightgcn-pytorch-from-scratch\n",
        "\n",
        "- https://github.com/gusye1234/LightGCN-PyTorch\n",
        "\n",
        "- https://github.com/SytzeAndr/NGCF_RP32/blob/hand-in/NGCF.ipynb\n"
      ],
      "metadata": {
        "id": "6RNPSfjWjV25"
      }
    }
  ]
}